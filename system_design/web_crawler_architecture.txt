http://stackoverflow.com/questions/5834808/designing-a-web-crawler

paper:
http://webpages.uncc.edu/sakella/courses/cloud09/papers/Mercator.pdf
http://research.microsoft.com/pubs/102936/eds-webcrawlerarchitecture.pdf
http://cis.poly.edu/suel/papers/crawl.pdf
http://ijcaonline.net/volume15/number7/pxc3872629.pdf

blog:
http://flexaired.blogspot.com/2011/09/design-web-crawler.html
https://www.semantics3.com/blog/2012/09/03/how-we-built-our-almost-distributed-web-crawler/
http://www.slideshare.net/poonamkenkre/web-crawler-14590800   *** good one


Web crawler:
1) 10 billion pages. 4 weeks for refresh, need to process 4000 pages per second
2) partition: by hostname. minimize network communication.
3) set-membership test. In memory? on-disk? disk seek 10ms.
4) frontier set. yet to be downloaded URLs.
    FIFO queue? Priority Queue? popularity of webpages are different.. Expensive disk IO.
    solution: 2 stages:
    front-end: prioritize URLs.
    back-end. enforce pliteness policy 
    both are composed of a number of parallel FIFO queues.


Q<>
L <- Q
download page from L. add to inverted index or store to db
parse L get list of N, add to Q.
